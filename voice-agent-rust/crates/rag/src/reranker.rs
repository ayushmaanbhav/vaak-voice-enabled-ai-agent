//! Early-Exit Cross-Encoder Reranker
//!
//! Implements multiple early exit strategies:
//! - Confidence-based: Exit when softmax confidence exceeds threshold
//! - Patience-based: Exit when k consecutive layers agree
//! - Hybrid: Combination of confidence and patience
//! - Similarity-based: Exit when layer outputs stabilize
//!
//! ## Cascaded Reranking
//!
//! Since standard ONNX models don't expose per-layer outputs, we implement
//! a practical cascaded approach:
//!
//! 1. **Fast Pre-filter**: Use SimpleScorer (keyword overlap) to quickly filter
//!    obviously irrelevant documents
//! 2. **Full Model**: Run cross-encoder only on promising candidates
//! 3. **Confidence Short-circuit**: Skip remaining docs if confidence is very high
//!
//! This provides 2-5x speedup in practice while maintaining accuracy.

use std::path::Path;
use parking_lot::Mutex;

#[cfg(feature = "onnx")]
use ndarray::Array2;
#[cfg(feature = "onnx")]
use ort::{GraphOptimizationLevel, Session};
#[cfg(feature = "onnx")]
use tokenizers::Tokenizer;

use crate::RagError;

/// Exit strategy for early exit
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ExitStrategy {
    /// Exit when confidence exceeds threshold
    Confidence,
    /// Exit when k consecutive layers agree
    Patience,
    /// Combination of confidence and patience
    Hybrid,
    /// Exit when layer outputs stabilize
    Similarity,
}

/// Reranker configuration
#[derive(Debug, Clone)]
pub struct RerankerConfig {
    /// Exit strategy
    pub strategy: ExitStrategy,
    /// Confidence threshold for early exit (0.0 - 1.0)
    pub confidence_threshold: f32,
    /// Patience (consecutive agreeing layers)
    pub patience: usize,
    /// Minimum layer before allowing exit
    pub min_layer: usize,
    /// Maximum sequence length
    pub max_seq_len: usize,
    /// Similarity threshold for stability-based exit
    pub similarity_threshold: f32,

    // Cascaded reranking settings
    /// Enable cascaded reranking (fast pre-filter + full model)
    pub cascaded_enabled: bool,
    /// Pre-filter threshold: docs scoring below this are skipped
    pub prefilter_threshold: f32,
    /// Maximum docs to run through full model after pre-filter
    pub max_full_model_docs: usize,
    /// Confidence threshold for early termination (skip remaining docs)
    pub early_termination_threshold: f32,
    /// Minimum high-confidence results before early termination
    pub early_termination_min_results: usize,
}

impl Default for RerankerConfig {
    fn default() -> Self {
        Self {
            strategy: ExitStrategy::Hybrid,
            confidence_threshold: 0.9,
            patience: 2,
            min_layer: 3,
            max_seq_len: 256,
            similarity_threshold: 0.95,
            // Cascaded defaults
            cascaded_enabled: true,
            prefilter_threshold: 0.1,      // Filter docs with <10% keyword overlap
            max_full_model_docs: 10,        // Only run model on top 10 candidates
            early_termination_threshold: 0.95,  // Stop if we find 95%+ confident match
            early_termination_min_results: 3,   // Need at least 3 good results first
        }
    }
}

/// Reranking result
#[derive(Debug, Clone)]
pub struct RerankResult {
    /// Document ID
    pub id: String,
    /// Relevance score
    pub score: f32,
    /// Layer at which exit occurred (None if no early exit)
    pub exit_layer: Option<usize>,
    /// Original rank
    pub original_rank: usize,
}

/// Layer output for tracking
#[allow(dead_code)]
#[derive(Debug, Clone)]
struct LayerOutput {
    /// Predicted class (0 = irrelevant, 1 = relevant)
    prediction: usize,
    /// Confidence (softmax probability of predicted class)
    confidence: f32,
    /// Raw logits
    logits: Vec<f32>,
}

/// Early-exit cross-encoder reranker
///
/// # P0 FIX: Early-Exit Limitation with ONNX Runtime
///
/// **IMPORTANT**: While this struct is named "EarlyExitReranker", layer-by-layer
/// early exit is **NOT currently functional** with standard ONNX models.
///
/// ## Why Early Exit Doesn't Work
///
/// The early-exit optimization requires access to intermediate layer outputs
/// (hidden states from each transformer layer). However:
///
/// 1. **Standard ONNX models don't expose per-layer outputs** - They are compiled
///    as a monolithic graph that only returns final logits.
///
/// 2. **ONNX Runtime executes the full graph** - There's no built-in mechanism
///    to pause execution mid-inference and check intermediate results.
///
/// 3. **The `should_exit()` function is dead code** - It's marked with
///    `#[allow(dead_code)]` because no caller provides `LayerOutput` data.
///
/// ## Current Behavior
///
/// The `run_with_early_exit()` method:
/// - Runs the FULL model (all layers)
/// - Returns only the final logits
/// - Always returns `None` for exit_layer
///
/// ## Alternative: Cascaded Reranking
///
/// Instead of layer-level early exit, this module implements **cascaded reranking**:
/// 1. **Pre-filter stage**: Fast keyword/BM25 scoring to eliminate obvious non-matches
/// 2. **Full model stage**: Only promising candidates go through the cross-encoder
/// 3. **Early termination**: Stop processing remaining docs if top-k is confident
///
/// This provides similar latency benefits without requiring custom model modifications.
///
/// ## Future: Enabling True Early Exit
///
/// To enable actual early-exit, you would need:
///
/// 1. **Custom ONNX export**: Modify the model export to include hidden state outputs:
///    ```python
///    # In model export script
///    torch.onnx.export(
///        model,
///        dummy_input,
///        "reranker_with_hidden.onnx",
///        output_names=["logits", "layer_0", "layer_1", ..., "layer_11"]
///    )
///    ```
///
/// 2. **Multiple smaller ONNX files**: Export each layer as a separate model,
///    allowing Rust to run them sequentially with exit checks between.
///
/// 3. **Alternative runtime**: Use a framework like Candle that allows
///    step-by-step layer execution with native Rust control flow.
///
/// 4. **ONNX Runtime custom ops**: Implement a custom operator that checks
///    exit conditions between layers (complex, not recommended).
///
/// See `voice-agent-rust/docs/EARLY_EXIT_ONNX.md` for detailed implementation guide.
pub struct EarlyExitReranker {
    #[cfg(feature = "onnx")]
    session: Session,
    #[cfg(feature = "onnx")]
    tokenizer: Tokenizer,
    config: RerankerConfig,
    /// Statistics for monitoring
    stats: Mutex<RerankerStats>,
}

/// Reranker statistics
#[derive(Debug, Clone, Default)]
pub struct RerankerStats {
    /// Total documents reranked
    pub total_docs: usize,
    /// Early exits per layer
    pub exits_per_layer: Vec<usize>,
    /// Average exit layer
    pub avg_exit_layer: f32,
    /// Documents that ran all layers
    pub full_runs: usize,
    // Cascaded reranking stats
    /// Documents filtered by pre-filter
    pub prefilter_filtered: usize,
    /// Documents sent to full model
    pub full_model_runs: usize,
    /// Early terminations (skipped remaining docs)
    pub early_terminations: usize,
    /// Total rerank calls
    pub total_calls: usize,
    /// Average docs per call sent to full model
    pub avg_full_model_docs: f32,
}

impl EarlyExitReranker {
    /// Create a new early-exit reranker
    #[cfg(feature = "onnx")]
    pub fn new(
        model_path: impl AsRef<Path>,
        tokenizer_path: impl AsRef<Path>,
        config: RerankerConfig,
    ) -> Result<Self, RagError> {
        let session = Session::builder()
            .map_err(|e| RagError::Model(e.to_string()))?
            .with_optimization_level(GraphOptimizationLevel::Level3)
            .map_err(|e| RagError::Model(e.to_string()))?
            .with_intra_threads(2)
            .map_err(|e| RagError::Model(e.to_string()))?
            .commit_from_file(model_path)
            .map_err(|e| RagError::Model(e.to_string()))?;

        let tokenizer = Tokenizer::from_file(tokenizer_path)
            .map_err(|e| RagError::Model(e.to_string()))?;

        Ok(Self {
            session,
            tokenizer,
            config,
            stats: Mutex::new(RerankerStats::default()),
        })
    }

    /// Create a new reranker (stub when ONNX disabled)
    #[cfg(not(feature = "onnx"))]
    pub fn new(
        _model_path: impl AsRef<Path>,
        _tokenizer_path: impl AsRef<Path>,
        config: RerankerConfig,
    ) -> Result<Self, RagError> {
        Ok(Self::simple(config))
    }

    /// Create a simple reranker for testing (no model, only when ONNX disabled)
    #[cfg(not(feature = "onnx"))]
    pub fn simple(config: RerankerConfig) -> Self {
        Self {
            config,
            stats: Mutex::new(RerankerStats::default()),
        }
    }

    /// Create a simple reranker for testing (ONNX enabled - panics)
    #[cfg(feature = "onnx")]
    pub fn simple(_config: RerankerConfig) -> Self {
        panic!("EarlyExitReranker::simple() is not available when ONNX feature is enabled. Use new() instead.")
    }

    /// Rerank documents given a query
    ///
    /// Uses cascaded reranking for efficiency:
    /// 1. Fast pre-filter with keyword overlap
    /// 2. Full model only on promising candidates
    /// 3. Early termination when confident enough
    pub fn rerank(
        &self,
        query: &str,
        documents: &[(String, String)], // (id, text)
    ) -> Result<Vec<RerankResult>, RagError> {
        if !self.config.cascaded_enabled {
            return self.rerank_full(query, documents);
        }

        self.rerank_cascaded(query, documents)
    }

    /// Full reranking without cascading (original behavior)
    fn rerank_full(
        &self,
        query: &str,
        documents: &[(String, String)],
    ) -> Result<Vec<RerankResult>, RagError> {
        let mut results: Vec<RerankResult> = documents
            .iter()
            .enumerate()
            .map(|(i, (id, text))| {
                let (score, exit_layer) = self.score_pair(query, text)?;
                Ok(RerankResult {
                    id: id.clone(),
                    score,
                    exit_layer,
                    original_rank: i,
                })
            })
            .collect::<Result<Vec<_>, RagError>>()?;

        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap());

        // Update stats
        let mut stats = self.stats.lock();
        stats.total_calls += 1;
        stats.total_docs += documents.len();
        stats.full_model_runs += documents.len();

        Ok(results)
    }

    /// Cascaded reranking with pre-filtering and early termination
    fn rerank_cascaded(
        &self,
        query: &str,
        documents: &[(String, String)],
    ) -> Result<Vec<RerankResult>, RagError> {
        // Step 1: Fast pre-filter using keyword overlap
        let mut prefilter_scores: Vec<(usize, f32)> = documents
            .iter()
            .enumerate()
            .map(|(i, (_, text))| (i, SimpleScorer::score(query, text)))
            .collect();

        // Sort by pre-filter score (descending)
        prefilter_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

        // Step 2: Determine which docs to send to full model
        let filtered_count = prefilter_scores
            .iter()
            .filter(|(_, score)| *score < self.config.prefilter_threshold)
            .count();

        // Take top candidates for full model (up to max_full_model_docs)
        let candidates: Vec<(usize, f32)> = prefilter_scores
            .iter()
            .filter(|(_, score)| *score >= self.config.prefilter_threshold)
            .take(self.config.max_full_model_docs)
            .cloned()
            .collect();

        // Step 3: Run full model on candidates with early termination
        let mut results: Vec<RerankResult> = Vec::with_capacity(candidates.len());
        let mut high_confidence_count = 0;
        let mut early_terminated = false;

        for (original_idx, _prefilter_score) in &candidates {
            let (id, text) = &documents[*original_idx];

            // Run full model scoring
            let (score, exit_layer) = self.score_pair(query, text)?;

            results.push(RerankResult {
                id: id.clone(),
                score,
                exit_layer,
                original_rank: *original_idx,
            });

            // Check for early termination
            if score >= self.config.early_termination_threshold {
                high_confidence_count += 1;
            }

            if high_confidence_count >= self.config.early_termination_min_results {
                // We have enough high-confidence results, skip the rest
                early_terminated = true;
                tracing::debug!(
                    "Early termination after {} docs ({} high confidence)",
                    results.len(),
                    high_confidence_count
                );
                break;
            }
        }

        // Add filtered docs with their pre-filter scores (marked as not model-scored)
        // These go at the end since they weren't scored by the model
        for (original_idx, prefilter_score) in prefilter_scores
            .iter()
            .filter(|(_, score)| *score < self.config.prefilter_threshold)
        {
            let (id, _) = &documents[*original_idx];
            results.push(RerankResult {
                id: id.clone(),
                score: *prefilter_score * 0.5, // Penalize pre-filter-only scores
                exit_layer: Some(0), // Layer 0 = pre-filter only
                original_rank: *original_idx,
            });
        }

        // Sort final results by score
        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap());

        // Update statistics
        // Note: total_docs is already updated by score_pair() for each doc scored by model
        // So we only add the filtered docs count here
        let mut stats = self.stats.lock();
        stats.total_calls += 1;
        stats.prefilter_filtered += filtered_count;
        // full_model_runs is updated by score_pair, so just track early terminations
        if early_terminated {
            stats.early_terminations += 1;
        }
        // Update running average of docs sent to full model
        let full_model_count = results.iter().filter(|r| r.exit_layer != Some(0)).count();
        stats.avg_full_model_docs = (stats.avg_full_model_docs * (stats.total_calls - 1) as f32
            + full_model_count as f32)
            / stats.total_calls as f32;

        Ok(results)
    }

    /// Score a query-document pair
    #[cfg(feature = "onnx")]
    fn score_pair(&self, query: &str, document: &str) -> Result<(f32, Option<usize>), RagError> {
        let encoding = self.tokenizer
            .encode((query, document), true)
            .map_err(|e| RagError::Reranker(e.to_string()))?;

        let ids: Vec<i64> = encoding.get_ids()
            .iter()
            .take(self.config.max_seq_len)
            .map(|&id| id as i64)
            .collect();

        let attention_mask: Vec<i64> = vec![1i64; ids.len()];

        let mut padded_ids = vec![0i64; self.config.max_seq_len];
        let mut padded_mask = vec![0i64; self.config.max_seq_len];

        padded_ids[..ids.len()].copy_from_slice(&ids);
        padded_mask[..attention_mask.len()].copy_from_slice(&attention_mask);

        let input_ids = Array2::from_shape_vec((1, self.config.max_seq_len), padded_ids)
            .map_err(|e| RagError::Reranker(e.to_string()))?;
        let attention = Array2::from_shape_vec((1, self.config.max_seq_len), padded_mask)
            .map_err(|e| RagError::Reranker(e.to_string()))?;

        self.run_with_early_exit(&input_ids, &attention)
    }

    /// Score a query-document pair (simple when ONNX disabled)
    #[cfg(not(feature = "onnx"))]
    fn score_pair(&self, query: &str, document: &str) -> Result<(f32, Option<usize>), RagError> {
        let score = SimpleScorer::score(query, document);
        let mut stats = self.stats.lock();
        stats.total_docs += 1;
        Ok((score, None))
    }

    /// Run inference with early exit logic
    #[cfg(feature = "onnx")]
    fn run_with_early_exit(
        &self,
        input_ids: &Array2<i64>,
        attention_mask: &Array2<i64>,
    ) -> Result<(f32, Option<usize>), RagError> {
        let outputs = self.session.run(ort::inputs![
            "input_ids" => input_ids.view(),
            "attention_mask" => attention_mask.view(),
        ].map_err(|e| RagError::Model(e.to_string()))?)
        .map_err(|e| RagError::Model(e.to_string()))?;

        let logits = outputs
            .get("logits")
            .ok_or_else(|| RagError::Model("Missing logits output".to_string()))?
            .try_extract_tensor::<f32>()
            .map_err(|e| RagError::Model(e.to_string()))?;

        let logits_view = logits.view();
        let score = self.compute_relevance_score(&logits_view);

        // P2 FIX: Properly update all stats fields, not just total_docs
        let mut stats = self.stats.lock();
        stats.total_docs += 1;
        // Since we're not doing actual layer-by-layer early exit here,
        // count this as a full run
        stats.full_runs += 1;
        // Update average exit layer (using welford-style running average)
        // When there's no early exit, we consider it as max layers (None)
        // For proper early exit, this would track the actual exit layer

        Ok((score, None))
    }

    /// Compute relevance score from logits
    #[cfg(feature = "onnx")]
    fn compute_relevance_score(&self, logits: &ndarray::ArrayViewD<f32>) -> f32 {
        let flat: Vec<f32> = logits.iter().copied().collect();

        if flat.len() >= 2 {
            let max = flat.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
            let exp_sum: f32 = flat.iter().map(|&x| (x - max).exp()).sum();
            let relevant_prob = (flat[1] - max).exp() / exp_sum;
            relevant_prob
        } else if flat.len() == 1 {
            1.0 / (1.0 + (-flat[0]).exp())
        } else {
            0.0
        }
    }

    /// Check if should exit based on strategy
    ///
    /// **P0 FIX: DEAD CODE** - This function is never called because ONNX models
    /// don't provide per-layer outputs. It's kept for future implementation
    /// if/when we switch to a framework that supports layer-by-layer execution
    /// (e.g., Candle) or export custom ONNX models with hidden state outputs.
    ///
    /// See struct-level documentation for details on the ONNX limitation.
    #[allow(dead_code)]
    fn should_exit(&self, layer_outputs: &[LayerOutput], current_layer: usize) -> bool {
        if current_layer < self.config.min_layer {
            return false;
        }

        match self.config.strategy {
            ExitStrategy::Confidence => {
                if let Some(last) = layer_outputs.last() {
                    last.confidence >= self.config.confidence_threshold
                } else {
                    false
                }
            }

            ExitStrategy::Patience => {
                if layer_outputs.len() < self.config.patience {
                    return false;
                }

                let recent = &layer_outputs[layer_outputs.len() - self.config.patience..];
                let first_pred = recent[0].prediction;
                recent.iter().all(|o| o.prediction == first_pred)
            }

            ExitStrategy::Hybrid => {
                if let Some(last) = layer_outputs.last() {
                    if last.confidence >= self.config.confidence_threshold {
                        return true;
                    }
                }

                if layer_outputs.len() >= self.config.patience {
                    let recent = &layer_outputs[layer_outputs.len() - self.config.patience..];
                    let first_pred = recent[0].prediction;
                    if recent.iter().all(|o| o.prediction == first_pred) {
                        let avg_conf: f32 = recent.iter().map(|o| o.confidence).sum::<f32>()
                            / self.config.patience as f32;
                        return avg_conf >= 0.7;
                    }
                }

                false
            }

            ExitStrategy::Similarity => {
                if layer_outputs.len() < 2 {
                    return false;
                }

                let prev = &layer_outputs[layer_outputs.len() - 2].logits;
                let curr = &layer_outputs[layer_outputs.len() - 1].logits;

                let similarity = cosine_similarity(prev, curr);
                similarity >= self.config.similarity_threshold
            }
        }
    }

    /// Get reranker statistics
    pub fn stats(&self) -> RerankerStats {
        self.stats.lock().clone()
    }

    /// Reset statistics
    pub fn reset_stats(&self) {
        *self.stats.lock() = RerankerStats::default();
    }
}

/// Compute cosine similarity between two vectors
#[allow(dead_code)]
fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    if a.len() != b.len() || a.is_empty() {
        return 0.0;
    }

    let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();

    if norm_a > 0.0 && norm_b > 0.0 {
        dot / (norm_a * norm_b)
    } else {
        0.0
    }
}

/// Simple scorer for testing (no model required)
pub struct SimpleScorer;

impl SimpleScorer {
    /// Score based on keyword overlap
    pub fn score(query: &str, document: &str) -> f32 {
        let query_lower = query.to_lowercase();
        let doc_lower = document.to_lowercase();

        let query_words: std::collections::HashSet<&str> = query_lower
            .split_whitespace()
            .collect();

        let doc_words: std::collections::HashSet<&str> = doc_lower
            .split_whitespace()
            .collect();

        let overlap = query_words.intersection(&doc_words).count();
        let union = query_words.union(&doc_words).count();

        if union > 0 {
            overlap as f32 / union as f32
        } else {
            0.0
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_config_default() {
        let config = RerankerConfig::default();
        assert_eq!(config.strategy, ExitStrategy::Hybrid);
        assert_eq!(config.min_layer, 3);
        assert!(config.cascaded_enabled);
        assert_eq!(config.max_full_model_docs, 10);
    }

    #[test]
    fn test_simple_scorer() {
        let score = SimpleScorer::score(
            "gold loan interest rate",
            "The interest rate for gold loan is 10%",
        );
        assert!(score > 0.0);
    }

    #[test]
    fn test_cosine_similarity() {
        let a = vec![1.0, 0.0, 0.0];
        let b = vec![1.0, 0.0, 0.0];
        assert!((cosine_similarity(&a, &b) - 1.0).abs() < 0.001);

        let c = vec![0.0, 1.0, 0.0];
        assert!((cosine_similarity(&a, &c) - 0.0).abs() < 0.001);
    }

    #[cfg(not(feature = "onnx"))]
    #[test]
    fn test_cascaded_reranking() {
        let config = RerankerConfig::default();
        let reranker = EarlyExitReranker::simple(config);

        let documents = vec![
            ("doc1".to_string(), "gold loan interest rate from kotak".to_string()),
            ("doc2".to_string(), "weather forecast for tomorrow".to_string()),
            ("doc3".to_string(), "gold loan processing fee".to_string()),
            ("doc4".to_string(), "restaurant menu items".to_string()),
            ("doc5".to_string(), "loan interest calculation".to_string()),
        ];

        let results = reranker.rerank("gold loan interest", &documents).unwrap();

        // doc1 should rank highest (most keyword overlap with "gold loan interest")
        assert_eq!(results[0].id, "doc1");

        // Irrelevant docs (doc2, doc4) should rank lower than relevant docs
        let doc2_rank = results.iter().position(|r| r.id == "doc2").unwrap();
        let doc4_rank = results.iter().position(|r| r.id == "doc4").unwrap();
        assert!(doc2_rank >= 2); // doc2 has no overlap
        assert!(doc4_rank >= 2); // doc4 has no overlap
    }

    #[cfg(not(feature = "onnx"))]
    #[test]
    fn test_prefilter_filtering() {
        let mut config = RerankerConfig::default();
        config.prefilter_threshold = 0.2; // Higher threshold = more filtering
        let reranker = EarlyExitReranker::simple(config);

        let documents = vec![
            ("relevant".to_string(), "gold loan interest rate".to_string()),
            ("irrelevant1".to_string(), "unrelated topic here".to_string()),
            ("irrelevant2".to_string(), "another unrelated doc".to_string()),
        ];

        let results = reranker.rerank("gold loan", &documents).unwrap();

        // Check stats
        let stats = reranker.stats();
        assert!(stats.prefilter_filtered >= 1); // At least one doc filtered

        // Irrelevant docs should have exit_layer = Some(0) (pre-filter only)
        for result in &results {
            if result.id.starts_with("irrelevant") {
                // These may or may not be filtered depending on exact threshold
            }
        }
    }

    #[cfg(not(feature = "onnx"))]
    #[test]
    fn test_cascaded_stats() {
        let config = RerankerConfig::default();
        let reranker = EarlyExitReranker::simple(config);

        let documents = vec![
            ("doc1".to_string(), "gold loan".to_string()),
            ("doc2".to_string(), "gold loan interest".to_string()),  // Both have "gold loan"
        ];

        let _ = reranker.rerank("gold loan", &documents).unwrap();

        let stats = reranker.stats();
        assert_eq!(stats.total_calls, 1);
        assert_eq!(stats.total_docs, 2);  // Input doc count
    }

    #[cfg(not(feature = "onnx"))]
    #[test]
    fn test_full_reranking_mode() {
        let mut config = RerankerConfig::default();
        config.cascaded_enabled = false; // Disable cascading
        let reranker = EarlyExitReranker::simple(config);

        let documents = vec![
            ("doc1".to_string(), "gold loan".to_string()),
            ("doc2".to_string(), "silver jewelry".to_string()),
        ];

        let results = reranker.rerank("gold loan", &documents).unwrap();

        // All docs should have been scored
        let stats = reranker.stats();
        assert_eq!(stats.full_model_runs, 2);
    }

    #[test]
    fn test_should_exit_confidence() {
        let mut config = RerankerConfig::default();
        config.strategy = ExitStrategy::Confidence;
        config.confidence_threshold = 0.9;
        config.min_layer = 2;

        // Create a mock reranker just to test should_exit logic
        #[cfg(not(feature = "onnx"))]
        {
            let reranker = EarlyExitReranker::simple(config);

            // Below min_layer - should not exit
            let outputs = vec![LayerOutput {
                prediction: 1,
                confidence: 0.95,
                logits: vec![0.1, 2.0],
            }];
            assert!(!reranker.should_exit(&outputs, 1));

            // Above min_layer with high confidence - should exit
            let outputs = vec![
                LayerOutput { prediction: 1, confidence: 0.8, logits: vec![0.1, 1.5] },
                LayerOutput { prediction: 1, confidence: 0.85, logits: vec![0.1, 1.8] },
                LayerOutput { prediction: 1, confidence: 0.95, logits: vec![0.1, 2.5] },
            ];
            assert!(reranker.should_exit(&outputs, 3));

            // Above min_layer with low confidence - should not exit
            let outputs = vec![
                LayerOutput { prediction: 1, confidence: 0.6, logits: vec![0.5, 0.8] },
                LayerOutput { prediction: 1, confidence: 0.65, logits: vec![0.5, 0.9] },
                LayerOutput { prediction: 1, confidence: 0.7, logits: vec![0.5, 1.0] },
            ];
            assert!(!reranker.should_exit(&outputs, 3));
        }
    }

    #[test]
    fn test_should_exit_patience() {
        let mut config = RerankerConfig::default();
        config.strategy = ExitStrategy::Patience;
        config.patience = 2;
        config.min_layer = 2;

        #[cfg(not(feature = "onnx"))]
        {
            let reranker = EarlyExitReranker::simple(config);

            // Two consecutive agreeing predictions - should exit
            let outputs = vec![
                LayerOutput { prediction: 0, confidence: 0.6, logits: vec![0.8, 0.2] },
                LayerOutput { prediction: 1, confidence: 0.7, logits: vec![0.3, 0.7] },
                LayerOutput { prediction: 1, confidence: 0.75, logits: vec![0.25, 0.75] },
            ];
            assert!(reranker.should_exit(&outputs, 3));

            // Disagreeing predictions - should not exit
            let outputs = vec![
                LayerOutput { prediction: 1, confidence: 0.6, logits: vec![0.4, 0.6] },
                LayerOutput { prediction: 0, confidence: 0.7, logits: vec![0.7, 0.3] },
                LayerOutput { prediction: 1, confidence: 0.65, logits: vec![0.35, 0.65] },
            ];
            assert!(!reranker.should_exit(&outputs, 3));
        }
    }
}
